{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport tensorflow as tf\nimport os",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2e3724b61bd52fba564c7ae1fd10b2e194cdb495"
      },
      "cell_type": "markdown",
      "source": "# 0.1 get_data.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c9d541b7a0f32ad65a8572e66d2c7699821fde2"
      },
      "cell_type": "code",
      "source": "# import tensorflow as tf\nimport collections\nimport os\nimport random\nimport pickle\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nfrom tqdm import tqdm\n\n# Step 1: Download the data.\nurl = 'http://mattmahoney.net/dc/'\n\ndef maybe_download(filename, expected_bytes):\n  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n  if not os.path.exists(filename):\n    filename, _ = urllib.request.urlretrieve(url + filename, filename)\n  statinfo = os.stat(filename)\n  if statinfo.st_size == expected_bytes:\n    print('Found and verified', filename)\n  else:\n    print(statinfo.st_size)\n    raise Exception(\n        'Failed to verify ' + filename + '. Can you get to it with a browser?')\n  return filename\n\nfilename = maybe_download('text8.zip', 31344016)\n\n# Read the data into a list of strings.\ndef read_data(filename):\n  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n  with zipfile.ZipFile(filename) as f:\n    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n  return data\n\nwords = read_data(filename)\nprint('Data size', len(words))\n\n# Step 2: Build the dictionary and replace rare words with UNK token.\nvocabulary_size = 50000\n\ndef build_dataset(words):\n  count = [['UNK', -1]]\n  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    if word in dictionary:\n      index = dictionary[word]\n    else:\n      index = 0  # dictionary['UNK']\n      unk_count += 1\n    data.append(index)\n  count[0][1] = unk_count\n  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n  return data, count, dictionary, reverse_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(words)\ndel words  # Hint to reduce memory.\n\ndata_index = 0\n# Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  for i in range(batch_size // num_skips):\n    target = skip_window  # target label at the center of the buffer\n    targets_to_avoid = [skip_window]\n    for j in range(num_skips):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[target]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  return batch, labels\n\nbatch_size = 128\nskip_window = 1       # How many words to consider left and right.\nnum_skips = 2         # How many times to reuse an input to generate a label.\n\n# We pick a random validation set to sample nearest neighbors. Here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent.\nvalid_size = 16     # Random set of words to evaluate similarity on.\nvalid_window = 100  # Only pick dev samples in the head of the distribution.\nvalid_examples = np.random.choice(valid_window, valid_size, replace=False)\n\n# Step 5: Begin training.\nnum_steps = 30000\ntraining_data = []\nfor step in tqdm(range(num_steps)):\n  batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n  training_data.append((batch_inputs, batch_labels))\n\ndata_folder = './data'\nif not os.path.exists(data_folder):\n  os.makedirs(data_folder)\nprint('Saving training data...')\npickle.dump(training_data, open( \"./data/train.p\", \"wb\" ))\nprint('Saving validation data...')\npickle.dump(valid_examples, open(\"./data/val.p\", \"wb\"))\nprint('Saving reverse_dictionary')\npickle.dump(reverse_dictionary, open(\"./data/reverse_dictionary.p\", \"wb\"))  ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found and verified text8.zip\nData size 17005207\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "100%|██████████| 30000/30000 [00:14<00:00, 2009.23it/s]\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Saving training data...\nSaving validation data...\nSaving reverse_dictionary\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78f85de0318968dd18efa3af220a235b76b83c17"
      },
      "cell_type": "code",
      "source": "os.listdir('./')",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "['__notebook_source__.ipynb', 'text8.zip', '.ipynb_checkpoints', 'data']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee05ad61622708b3910212837902e8ff68ae7902"
      },
      "cell_type": "code",
      "source": "os.listdir('./data/')",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "['reverse_dictionary.p', 'train.p', 'val.p']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c3fb7c600338e226ca3114a373d55757b7ff0b0"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "387d3511873401eee6c0ea0fd5850ce8376e2784"
      },
      "cell_type": "markdown",
      "source": "# 0.2 utils.py"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9baad59e26b3f857a60138886d0910819312ac2c"
      },
      "cell_type": "code",
      "source": "import matplotlib\n# matplotlib.use('TKAgg')\nfrom matplotlib import pyplot as plt\nimport pickle\nimport numpy as np\n\nfrom sklearn.manifold import TSNE\n\ndef load_data():\n    train_data_path = './data/train.p'\n    val_data_path = './data/val.p'\n    reverse_dictionary_path = './data/reverse_dictionary.p'\n\n    train_data = pickle.load(open(train_data_path, 'rb'))\n    print(\"Loaded train data!\")\n    val_data = pickle.load(open(val_data_path, 'rb'))\n    print(\"Loaded val data!\")\n    reverse_dictionary = pickle.load(open(reverse_dictionary_path, 'rb'))\n    print(\"Loaded reverse dictionary!\")\n    return train_data, val_data, reverse_dictionary\n\ndef print_closest_words(val_index, nearest, reverse_dictionary):\n    val_word = reverse_dictionary[val_index]                 \n    log_str = \"Nearest to %s:\" % val_word                          \n    for k in xrange(len(nearest)):                                        \n        close_word = reverse_dictionary[nearest[k]]                \n        log_str = \"%s %s,\" % (log_str, close_word)                 \n    print(log_str)\n\ndef plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n  assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n  plt.figure(figsize=(18, 18))  # in inches\n  for i, label in enumerate(labels):\n    x, y = low_dim_embs[i, :]\n    plt.scatter(x, y)\n    plt.annotate(label,\n                 xy=(x, y),\n                 xytext=(5, 2),\n                 textcoords='offset points',\n                 ha='right',\n                 va='bottom')\n\n  plt.savefig(filename)\n\ndef visualize_embeddings(final_embeddings, reverse_dictionary):\n    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n    plot_only = 500\n    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n    plot_with_labels(low_dim_embs, labels)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8c0a400fdf3fc3c8237db4876e727ce9f52dcc6"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "52fd6104763d6c5d838f5a6f121f7770fbbfb91f"
      },
      "cell_type": "markdown",
      "source": "# 1. word2vec"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e7342e3ee988b707256c9d991176b5c3072f60f"
      },
      "cell_type": "code",
      "source": "import math\nimport numpy as np\nimport tensorflow as tf\n# from utils import *\n'''\nConsider the following sentence:\n\"the first cs224n homework was a lot of fun\"\nWith a window size of 1, we have the dataset:\n([the, cs224n], first), ([lot, fun], of) ...\nRemember that Skipgram tries to predict each context word from \nits target word, and so the task becomes to predict 'the' and\n'cs224n' from first, 'lot' and 'fun' from 'of' and so on.\nOur dataset now becomes:\n(first, the), (first, cs224n), (of, lot), (of, fun) ...\n'''\n# Let's define some constants first\nbatch_size = 128\nvocabulary_size = 50000\nembedding_size = 128  # Dimension of the embedding vector.\nnum_sampled = 64    # Number of negative examples to sample.\n\n'''\nload_data loads the already preprocessed training and val data.\ntrain data is a list of (batch_input, batch_labels) pairs.\nval data is a list of all validation inputs.\nreverse_dictionary is a python dict from word index to word\n'''\ntrain_data, val_data, reverse_dictionary = load_data()\nprint(\"Number of training examples:\", len(train_data)*batch_size)\nprint(\"Number of validation examples:\", len(val_data))\n\ndef skipgram():\n    batch_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n    batch_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    val_dataset = tf.constant(val_data, dtype=tf.int32)\n\n    with tf.variable_scope('word2vec') as scope:\n        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, \n                                                    embedding_size], \n                                                    -1.0, 1.0))\n        batch_embeddings = tf.nn.embedding_lookup(embeddings, batch_inputs)\n\n        weights = tf.Variable(tf.truncated_normal([vocabulary_size, \n                                                   embedding_size],\n                                                   stddev=1.0/math.sqrt(embedding_size)))\n        biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n        # This objective is maximized when the model assigns high probabilities\n        # to the real words, and low probabilities to noise words.\n        loss = tf.reduce_mean(tf.nn.nce_loss(weights=weights, \n                                             biases=biases,\n                                             labels=batch_labels,\n                                             inputs=batch_embeddings,\n                                             num_sampled=num_sampled,\n                                             num_classes=vocabulary_size))\n\n\n        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n        normalized_embeddings = embeddings/norm\n        \n        val_embeddings = tf.nn.embedding_lookup(normalized_embeddings, \n                                                val_dataset)\n        similarity = tf.matmul(val_embeddings, \n                               normalized_embeddings, transpose_b=True)\n\n    return batch_inputs, batch_labels, normalized_embeddings, similarity, loss\n\ndef run():\n    # load model\n    batch_inputs, batch_labels, normalized_embeddings, similarity, loss = skipgram()\n    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n    init = tf.global_variables_initializer()\n    with tf.Session() as session:\n        session.run(init)\n\n        average_loss = 0\n        for step, batch_data in enumerate(train_data):\n            inputs, labels = batch_data\n            feed_dict = {batch_inputs: inputs, batch_labels: labels}\n\n            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n            average_loss += loss_val\n            \n            if step % 1000 == 0:\n                if step > 0:\n                    average_loss /= 1000\n                print(\"Average loss at step \", step, \": \", average_loss)\n                average_loss = 0\n                    \n            if step % 5000 == 0:\n                sim = similarity.eval()\n                for i in xrange(len(val_data)):\n                    top_k = 8  # number of nearest neighbors                       \n                    nearest = (-sim[i, :]).argsort()[1:top_k + 1] \n                    print_closest_words(val_data[i], nearest, reverse_dictionary)\n  \n        final_embeddings = normalized_embeddings.eval()\n        return final_embeddings\n\n# Let's start training\nfinal_embeddings = run()\n\n# Visualize the embeddings.\nvisualize_embeddings(final_embeddings, reverse_dictionary)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73bae378e3e43492e1f9a5a40dc0f7a87b8592bb"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "91a8a4de994629e31be59c1fcedc8bed6daf74f2"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d696934e30311be34797b2bb20a515802fdbf74"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b71428dbbe69ff465919fe710fc79586c736804"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0758e8b96642c38f7e3d2347cc99259e3c64b9b1"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da79d83ef420980cf457ef8dac713e330a5d18c6"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}