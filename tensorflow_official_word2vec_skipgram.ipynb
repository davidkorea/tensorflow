{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport os\nimport sys\nimport argparse\nimport random\nfrom tempfile import gettempdir\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\n# Give a folder path as an argument with '--log_dir' to save\n# TensorBoard summaries. Default is a log folder in current directory.\ncurrent_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    '--log_dir',\n    type=str,\n    default=os.path.join(current_path, 'log'),\n    help='The log directory for TensorBoard summaries.')\nFLAGS, unparsed = parser.parse_known_args()\n\n# Create the directory for TensorBoard variables if there is not.\nif not os.path.exists(FLAGS.log_dir):\n  os.makedirs(FLAGS.log_dir)\n\n# Step 1: Download the data.\nurl = 'http://mattmahoney.net/dc/'\n\n\n# pylint: disable=redefined-outer-name\ndef maybe_download(filename, expected_bytes):\n  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n  local_filename = os.path.join(gettempdir(), filename)\n  if not os.path.exists(local_filename):\n    local_filename, _ = urllib.request.urlretrieve(url + filename,\n                                                   local_filename)\n  statinfo = os.stat(local_filename)\n  if statinfo.st_size == expected_bytes:\n    print('Found and verified', filename)\n  else:\n    print(statinfo.st_size)\n    raise Exception('Failed to verify ' + local_filename +\n                    '. Can you get to it with a browser?')\n  return local_filename\n\n\nfilename = maybe_download('text8.zip', 31344016)\n\n\n# Read the data into a list of strings.\ndef read_data(filename):\n  \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n  with zipfile.ZipFile(filename) as f:\n    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n  return data\n\n\nvocabulary = read_data(filename)\nprint('Data size', len(vocabulary))\n\n# Step 2: Build the dictionary and replace rare words with UNK token.\nvocabulary_size = 50000\n\n\ndef build_dataset(words, n_words):\n  \"\"\"Process raw inputs into a dataset.\"\"\"\n  count = [['UNK', -1]]\n  count.extend(collections.Counter(words).most_common(n_words - 1))\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    index = dictionary.get(word, 0)\n    if index == 0:  # dictionary['UNK']\n      unk_count += 1\n    data.append(index)\n  count[0][1] = unk_count\n  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n  return data, count, dictionary, reversed_dictionary\n\n\n# Filling 4 global variables:\n# data - list of codes (integers from 0 to vocabulary_size-1).\n#   This is the original text but words are replaced by their codes\n# count - map of words(strings) to count of occurrences\n# dictionary - map of words(strings) to their codes(integers)\n# reverse_dictionary - maps codes(integers) to words(strings)\ndata, count, dictionary, reverse_dictionary = build_dataset(\n    vocabulary, vocabulary_size)\ndel vocabulary  # Hint to reduce memory.\nprint('Most common words (+UNK)', count[:5])\nprint('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n\ndata_index = 0\n\n\n# Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n  if data_index + span > len(data):\n    data_index = 0\n  buffer.extend(data[data_index:data_index + span])\n  data_index += span\n  for i in range(batch_size // num_skips):\n    context_words = [w for w in range(span) if w != skip_window]\n    words_to_use = random.sample(context_words, num_skips)\n    for j, context_word in enumerate(words_to_use):\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[context_word]\n    if data_index == len(data):\n      buffer.extend(data[0:span])\n      data_index = span\n    else:\n      buffer.append(data[data_index])\n      data_index += 1\n  # Backtrack a little bit to avoid skipping words in the end of a batch\n  data_index = (data_index + len(data) - span) % len(data)\n  return batch, labels\n\n\nbatch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\nfor i in range(8):\n  print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n        reverse_dictionary[labels[i, 0]])\n\n# Step 4: Build and train a skip-gram model.\n\nbatch_size = 128\nembedding_size = 128  # Dimension of the embedding vector.\nskip_window = 1  # How many words to consider left and right.\nnum_skips = 2  # How many times to reuse an input to generate a label.\nnum_sampled = 64  # Number of negative examples to sample.\n\n# We pick a random validation set to sample nearest neighbors. Here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent. These 3 variables are used only for\n# displaying model accuracy, they don't affect calculation.\nvalid_size = 16  # Random set of words to evaluate similarity on.\nvalid_window = 100  # Only pick dev samples in the head of the distribution.\nvalid_examples = np.random.choice(valid_window, valid_size, replace=False)\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  with tf.name_scope('inputs'):\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n  # Ops and variables pinned to the CPU because of missing GPU implementation\n  with tf.device('/cpu:0'):\n    # Look up embeddings for inputs.\n    with tf.name_scope('embeddings'):\n      embeddings = tf.Variable(\n          tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n      embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n    # Construct the variables for the NCE loss\n    with tf.name_scope('weights'):\n      nce_weights = tf.Variable(\n          tf.truncated_normal(\n              [vocabulary_size, embedding_size],\n              stddev=1.0 / math.sqrt(embedding_size)))\n    with tf.name_scope('biases'):\n      nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n  # Compute the average NCE loss for the batch.\n  # tf.nce_loss automatically draws a new sample of the negative labels each\n  # time we evaluate the loss.\n  # Explanation of the meaning of NCE loss:\n  #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n  with tf.name_scope('loss'):\n    loss = tf.reduce_mean(\n        tf.nn.nce_loss(\n            weights=nce_weights,\n            biases=nce_biases,\n            labels=train_labels,\n            inputs=embed,\n            num_sampled=num_sampled,\n            num_classes=vocabulary_size))\n\n  # Add the loss value as a scalar to summary.\n  tf.summary.scalar('loss', loss)\n\n  # Construct the SGD optimizer using a learning rate of 1.0.\n  with tf.name_scope('optimizer'):\n    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n  # Compute the cosine similarity between minibatch examples and all embeddings.\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n  normalized_embeddings = embeddings / norm\n  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n                                            valid_dataset)\n  similarity = tf.matmul(\n      valid_embeddings, normalized_embeddings, transpose_b=True)\n\n  # Merge all summaries.\n  merged = tf.summary.merge_all()\n\n  # Add variable initializer.\n  init = tf.global_variables_initializer()\n\n  # Create a saver.\n  saver = tf.train.Saver()\n\n# Step 5: Begin training.\nnum_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n  # Open a writer to write summaries.\n  writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n\n  # We must initialize all variables before we use them.\n  init.run()\n  print('Initialized')\n\n  average_loss = 0\n  for step in xrange(num_steps):\n    batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n                                                skip_window)\n    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n\n    # Define metadata variable.\n    run_metadata = tf.RunMetadata()\n\n    # We perform one update step by evaluating the optimizer op (including it\n    # in the list of returned values for session.run()\n    # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n    # Feed metadata variable to session for visualizing the graph in TensorBoard.\n    _, summary, loss_val = session.run(\n        [optimizer, merged, loss],\n        feed_dict=feed_dict,\n        run_metadata=run_metadata)\n    average_loss += loss_val\n\n    # Add returned summaries to writer in each step.\n    writer.add_summary(summary, step)\n    # Add metadata to visualize the graph for the last run.\n    if step == (num_steps - 1):\n      writer.add_run_metadata(run_metadata, 'step%d' % step)\n\n    if step % 2000 == 0:\n      if step > 0:\n        average_loss /= 2000\n      # The average loss is an estimate of the loss over the last 2000 batches.\n      print('Average loss at step ', step, ': ', average_loss)\n      average_loss = 0\n\n    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n    if step % 10000 == 0:\n      sim = similarity.eval()\n      for i in xrange(valid_size):\n        valid_word = reverse_dictionary[valid_examples[i]]\n        top_k = 8  # number of nearest neighbors\n        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n        log_str = 'Nearest to %s:' % valid_word\n        for k in xrange(top_k):\n          close_word = reverse_dictionary[nearest[k]]\n          log_str = '%s %s,' % (log_str, close_word)\n        print(log_str)\n  final_embeddings = normalized_embeddings.eval()\n\n  # Write corresponding labels for the embeddings.\n  with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n    for i in xrange(vocabulary_size):\n      f.write(reverse_dictionary[i] + '\\n')\n\n  # Save the model for checkpoints.\n  saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n\n  # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n  config = projector.ProjectorConfig()\n  embedding_conf = config.embeddings.add()\n  embedding_conf.tensor_name = embeddings.name\n  embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n  projector.visualize_embeddings(writer, config)\n\nwriter.close()\n\n# Step 6: Visualize the embeddings.\n\n\n# pylint: disable=missing-docstring\n# Function to draw visualization of distance between embeddings.\ndef plot_with_labels(low_dim_embs, labels, filename):\n  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n  plt.figure(figsize=(18, 18))  # in inches\n  for i, label in enumerate(labels):\n    x, y = low_dim_embs[i, :]\n    plt.scatter(x, y)\n    plt.annotate(\n        label,\n        xy=(x, y),\n        xytext=(5, 2),\n        textcoords='offset points',\n        ha='right',\n        va='bottom')\n\n  plt.savefig(filename)\n\n\ntry:\n  # pylint: disable=g-import-not-at-top\n  from sklearn.manifold import TSNE\n  import matplotlib.pyplot as plt\n\n  tsne = TSNE(\n      perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n  plot_only = 500\n  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n  plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n\nexcept ImportError as ex:\n  print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n  print(ex)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}